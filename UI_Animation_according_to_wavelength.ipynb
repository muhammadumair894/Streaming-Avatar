{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashar-riaz/Streaming-Avatar/blob/main/UI_Animation_according_to_wavelength.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPMsmmJ5tW_X",
        "outputId": "e6deb635-aa71-4ac2-9f7e-854445da636d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.31.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpVwBenxtb5q",
        "outputId": "d0c8ad50-7cac-454d-9936-eec88ac6e66a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting aspose-words\n",
            "  Downloading aspose_words-24.6.0-py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: aspose-words\n"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install aspose-words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXEFz3OMtdX_"
      },
      "outputs": [],
      "source": [
        "!pip install transformers --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yoRhtGTOtgn6",
        "outputId": "889507c1-305f-4db8-9ce3-def14c59aa87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V8sDHqyvx7g"
      },
      "outputs": [],
      "source": [
        "Atext=input(\"Enter your text: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdsHqL_MuN1j"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "device = \"cuda\" # the device to load the model onto\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
        "\n",
        "prompt = Atext\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOE1Kbcuu65X"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfA-r4Fyv6cJ"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "import soundfile as sf\n",
        "\n",
        "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\")\n",
        "\n",
        "# Replace this with your custom speaker embedding\n",
        "custom_speaker_embedding = torch.randn(1, 512)  # Example of a random speaker embedding\n",
        "max_length = 512  # You might need to adjust this value\n",
        "truncated_response = response[:max_length]\n",
        "\n",
        "speech = synthesiser(truncated_response, forward_params={\"speaker_embeddings\": custom_speaker_embedding})\n",
        "\n",
        "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPQBssWbv9KG"
      },
      "outputs": [],
      "source": [
        "import IPython.display as ipd\n",
        "\n",
        "ipd.Audio(\"speech.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0Sx0EGpv_UM"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to check if an image is loaded correctly\n",
        "def check_image_loaded(image, image_path):\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Image at path {image_path} could not be loaded.\")\n",
        "    return image\n",
        "\n",
        "# Load images\n",
        "avatar_image_path = \"/content/face.png\"\n",
        "mouth_image_path_1 = \"/content/mouth.png\"\n",
        "\n",
        "avatar_image = cv2.imread(avatar_image_path)\n",
        "mouth_image_1 = cv2.imread(mouth_image_path_1)\n",
        "\n",
        "avatar_image = check_image_loaded(avatar_image, avatar_image_path)\n",
        "mouth_image_1 = check_image_loaded(mouth_image_1, mouth_image_path_1)\n",
        "\n",
        "# Print sizes to verify\n",
        "print(\"Avatar image shape:\", avatar_image.shape)\n",
        "print(\"Mouth image shape:\", mouth_image_1.shape)\n",
        "\n",
        "# Find the smallest dimensions\n",
        "min_height = min(avatar_image.shape[0], mouth_image_1.shape[0])\n",
        "min_width = min(avatar_image.shape[1], mouth_image_1.shape[1])\n",
        "\n",
        "# Resize images to the smallest dimensions\n",
        "avatar_image_resized = cv2.resize(avatar_image, (min_width, min_height))\n",
        "mouth_image_resized = cv2.resize(mouth_image_1, (min_width, min_height))\n",
        "\n",
        "# Save the resized images\n",
        "cv2.imwrite(\"avatar_image_reshaped.jpg\", avatar_image_resized)\n",
        "cv2.imwrite(\"mouth_image_reshaped.jpg\", mouth_image_resized)\n",
        "\n",
        "# Display the resized images\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(cv2.cvtColor(avatar_image_resized, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"Resized Avatar Image\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(cv2.cvtColor(mouth_image_resized, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"Resized Mouth Image\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71oUBh66wCO5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import librosa\n",
        "import moviepy.editor as mp\n",
        "from IPython.display import display, Video\n",
        "\n",
        "# Function to check if an image is loaded correctly\n",
        "def check_image_loaded(image, image_path):\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Image at path {image_path} could not be loaded.\")\n",
        "    return image\n",
        "\n",
        "# Load the images\n",
        "avatar_image_path = \"/content/avatar_image_reshaped.jpg\"\n",
        "mouth_open_path = \"/content/mouth_image_reshaped.jpg\"\n",
        "\n",
        "avatar_image = cv2.imread(cv2.samples.findFile(avatar_image_path))\n",
        "mouth_open_image = cv2.imread(cv2.samples.findFile(mouth_open_path))\n",
        "\n",
        "avatar_image = check_image_loaded(avatar_image, avatar_image_path)\n",
        "mouth_open_image = check_image_loaded(mouth_open_image, mouth_open_path)\n",
        "\n",
        "# Ensure both images are the same size\n",
        "target_size = min(avatar_image.shape[:2], mouth_open_image.shape[:2])\n",
        "avatar_image = cv2.resize(avatar_image, (target_size[1], target_size[0]))\n",
        "mouth_open_image = cv2.resize(mouth_open_image, (target_size[1], target_size[0]))\n",
        "\n",
        "# Load the audio file\n",
        "audio_file = \"/content/speech.wav\"\n",
        "audio, sample_rate = librosa.load(audio_file)\n",
        "\n",
        "# Extract the amplitude envelope\n",
        "amplitude_envelope = np.abs(librosa.core.amplitude_to_db(audio))\n",
        "\n",
        "# Create a video writer\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "video_writer = cv2.VideoWriter(\"lipsync.mp4\", fourcc, 30, (avatar_image.shape[1], avatar_image.shape[0]))\n",
        "\n",
        "# Iterate over the audio frames and update the avatar's mouth shape\n",
        "frame_duration = 1.0 / 30  # Duration of each video frame\n",
        "total_frames = int(len(audio) / (sample_rate * frame_duration))\n",
        "\n",
        "for i in range(total_frames):\n",
        "    start_sample = int(i * sample_rate * frame_duration)\n",
        "    end_sample = int((i + 1) * sample_rate * frame_duration)\n",
        "    frame_audio = audio[start_sample:end_sample]\n",
        "\n",
        "    # Determine the mouth shape based on the amplitude\n",
        "    if np.mean(np.abs(frame_audio)) > 0.025:  # Adjust the threshold as needed\n",
        "        frame = mouth_open_image\n",
        "    else:\n",
        "        frame = avatar_image\n",
        "\n",
        "    # Write the frame to the video\n",
        "    video_writer.write(frame)\n",
        "\n",
        "# Release the video writer\n",
        "video_writer.release()\n",
        "\n",
        "# Add audio to the video\n",
        "video = mp.VideoFileClip(\"lipsync.mp4\")\n",
        "audio = mp.AudioFileClip(audio_file)\n",
        "final_video = video.set_audio(audio)\n",
        "final_video.write_videofile(\"lipsync_with_audio.mp4\", codec='libx264')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykPFShWkwFIo"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Video\n",
        "\n",
        "# Assuming the video file is named 'lipsync_with_audio.mp4'\n",
        "video_path = 'lipsync_with_audio.mp4'  # Replace with the actual name of your uploaded file\n",
        "display(Video(video_path, embed=True, width=600, height=400))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcyewl10xSkd"
      },
      "outputs": [],
      "source": [
        "!pip install gtts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WX39xgTeviAq"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9PjMGzoTm5u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBhXUY-FvoWo"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from gtts import gTTS\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import librosa\n",
        "import moviepy.editor as mp\n",
        "from IPython.display import display, Video\n",
        "def avtar(audio_path):\n",
        "    # Function to check if an image is loaded correctly\n",
        "    def check_image_loaded(image, image_path):\n",
        "        if image is None:\n",
        "            raise FileNotFoundError(f\"Image at path {image_path} could not be loaded.\")\n",
        "        return image\n",
        "\n",
        "    # Load the images\n",
        "    avatar_image_path = \"avatar_image_reshaped.jpg\"\n",
        "    mouth_open_path = \"mouth_image_reshaped.jpg\"\n",
        "\n",
        "    avatar_image = cv2.imread(cv2.samples.findFile(avatar_image_path))\n",
        "    mouth_open_image = cv2.imread(cv2.samples.findFile(mouth_open_path))\n",
        "\n",
        "    avatar_image = check_image_loaded(avatar_image, avatar_image_path)\n",
        "    mouth_open_image = check_image_loaded(mouth_open_image, mouth_open_path)\n",
        "\n",
        "    # Ensure both images are the same size\n",
        "    target_size = min(avatar_image.shape[:2], mouth_open_image.shape[:2])\n",
        "    avatar_image = cv2.resize(avatar_image, (target_size[1], target_size[0]))\n",
        "    mouth_open_image = cv2.resize(mouth_open_image, (target_size[1], target_size[0]))\n",
        "\n",
        "    # Load the audio file\n",
        "    audio, sample_rate = librosa.load(audio_path)\n",
        "\n",
        "    # Extract the amplitude envelope\n",
        "    amplitude_envelope = np.abs(librosa.core.amplitude_to_db(audio))\n",
        "\n",
        "    # Create a video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    video_writer = cv2.VideoWriter(\"lipsync.mp4\", fourcc, 30, (avatar_image.shape[1], avatar_image.shape[0]))\n",
        "\n",
        "    # Iterate over the audio frames and update the avatar's mouth shape\n",
        "    frame_duration = 1.0 / 30  # Duration of each video frame\n",
        "    total_frames = int(len(audio) / (sample_rate * frame_duration))\n",
        "\n",
        "    for i in range(total_frames):\n",
        "        start_sample = int(i * sample_rate * frame_duration)\n",
        "        end_sample = int((i + 1) * sample_rate * frame_duration)\n",
        "        frame_audio = audio[start_sample:end_sample]\n",
        "\n",
        "        # Determine the mouth shape based on the amplitude\n",
        "        if np.mean(np.abs(frame_audio)) > 0.025:  # Adjust the threshold as needed\n",
        "            frame = mouth_open_image\n",
        "        else:\n",
        "            frame = avatar_image\n",
        "\n",
        "        # Write the frame to the video\n",
        "        video_writer.write(frame)\n",
        "\n",
        "    # Release the video writer\n",
        "    video_writer.release()\n",
        "\n",
        "    # Add audio to the video\n",
        "    video = mp.VideoFileClip(\"lipsync.mp4\")\n",
        "    audio = mp.AudioFileClip(audio_path)\n",
        "    final_video = video.set_audio(audio)\n",
        "    final_video.write_videofile(\"lipsync_with_audio.mp4\", codec='libx264')\n",
        "\n",
        "    return \"lipsync_with_audio.mp4\"\n",
        "def text_to_speech(text):\n",
        "    # Generate speech from text\n",
        "    tts = gTTS(text)\n",
        "    audio_path = \"response.mp3\"\n",
        "    tts.save(audio_path)\n",
        "    return audio_path\n",
        "\n",
        "def generate_response(input_text):\n",
        "    prompt = input_text\n",
        "    messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        model_inputs.input_ids,\n",
        "        max_new_tokens=512\n",
        "    )\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    audio_path = text_to_speech(response)\n",
        "\n",
        "        # Generate the avatar video\n",
        "    video_path = avtar(audio_path)\n",
        "\n",
        "    return response,audio_path,video_path\n",
        "\n",
        "\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=generate_response,\n",
        "    inputs=\"text\",\n",
        "    outputs=[\"text\",\"audio\",\"video\"],\n",
        "    title=\"Text to Avatar Speech\",\n",
        "    description=\"Enter text and get a spoken response by the avatar.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}